\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{siunitx}
\newtheorem{summary}{Summary}


<<echo=F>>=
  options(width=65)
@ 

\SweaveOpts{fig.width=6.5, fig.height=5, fig.align='center',
  out.width='0.95\\textwidth', cache=T}



\title{Web Resource Analysis Log}
\author{Simon Schubert}

\begin{document}

\maketitle


\section{Initialization}

For the time being, we work with a reduced data set of 2000 domains
(instead of 20k or 100k).

<<>>=
require(ggplot2)
require(plyr)
data(webresource)
@

\section{Wed May 30 16:31:25 CEST 2012}
\label{sec:wed-may-30}


Trying to more clearly define what I have been trying to visualize.

\subsection{What is the typical size of a website?}
\label{sec:what-typical-size}


Size has many dimensions.  Let's look first at:

\begin{itemize}
\item number of resources (and their types)
\item transferred size
\end{itemize}

Many of these metrics are aggregates, i.e. they have to be derived
from the set of requests per domain.

\subsubsection{Size in bytes}
\label{sec:size-bytes}


<<warning=F>>=
lenagg <- aggregate(encodedDataLength ~ domainId, webresource, sum, na.rm=T)
attach(lenagg)
lenagg$ecdf <- ecdf(encodedDataLength)(encodedDataLength)
qplot(encodedDataLength, data=lenagg, geom="density", log="x")
qplot(encodedDataLength, ecdf, data=lenagg, geom="line", log="x")
quantile(encodedDataLength, seq(0, 1, 0.05))
detach()
@ 

\begin{summary}
  Web sites are unexpecedly heavy in size: $2/3$ of the websites are
  larger than \SI{500}{\kilo\byte} and over $1/3$ is larger than
  \SI{1}{\mega\byte}.  The typical web site size is around \SI{750}{\kilo\byte}.
\end{summary}


\subsubsection{Number of resources}
\label{sec:number-resources}

<<warning=F>>=
## only count non-redirect transfers
resagg <- as.data.frame(table(webresource$domainId[webresource$redirect==F]), responseName="requests")
attach(resagg)
resagg$ecdf <- ecdf(requests)(requests)
qplot(requests, ecdf, data=resagg, geom="line", xlim=c(0, 200))
qplot(requests, data=resagg, geom="density", xlim=c(0,200), adjust=1/3)
quantile(requests, seq(0, 1, 0.1))
detach()
@ 

\begin{summary}
  There is no "typical" number of resources in a website;  instead,
  between 1 and 100 resources are almost uniformly distributed, with
  slight peaks at the low end ($\le 3$) and around 50 resources.
  Half of all web sites contain more than 70 resources, and almost
  \SI{10}{\percent} use more than 200 resources.  Only
  \SI{10}{\percent} load less than 15 resources.
\end{summary}


\appendix

\section{Random analysis ideas}

\subsection{Tree/branching structure of resources}
\label{sec:treebr-struct-reso}



Not all resources for a page are known to the browser at the initial
load time, nor after fetching the main HTML file.  Resources might be
referenced/included by other resources;  for example JS might
dynamically (at load time) create requests for images, etc.  The taller
this tree, the more sequential transfers, which impacts load efficiency
and speed, which in turn requires radios to stay active longer, etc.


\subsection{Differential analysis between vanilla and ad-block runs}
\label{sec:diff-analys-betw}



When we create a differential view between both runs, we can tag of
ad-related resources.  We then can operate off the vanilla run (which
includes tagged resources), and we don't have to work with two separate
data sets.


\subsection{Which hosts (of ads) are actually ad network hosts}
\label{sec:which-hosts-of}



To circumvent same-origin policy restrictions, domains (allegedly) use
canonical domain names for their ad servers, which are actually an ad
network's servers.  E.g. ads.nytimes.com might actually be
ads.doubleclick.net, but by using the nytimes.com domain, cookies can be
passed to the ad network.  We can look at this by performing a DNS CNAME
analysis, or even reverse DNS and/or whois queries for the IP block.


\subsection{Cookies}
\label{sec:cookies}



This could be a big area:  which page items, validity duration, which
domains, size of stored data.  Which cookie names match across domains,
which content matches across domains (indication for either similar
software or even aliased services).


\subsection{GET request parameters}
\label{sec:get-requ-param}



I've see very elaborate parametrized requests, which essentially force
the browser to always fetch a new copy and never operate from the
cache.  I'm a bit unclear on how/what to analyze there, but there are
many of those for sure.


\subsection{Run separate perf instance in parallel with the logging instance (don't
  profile logging instance, as we have it now)}
\label{sec:run-separate-perf}



This allows a more accurate assessment of the performance metrics (no
logging overhead).  We then can try to correlate aggregate domain
metrics with aggregate performance counter values and maybe create a
relation between these metrics.  This is a statistical method, and might
not lead to any reasonable insight.  But maybe it can turn out that
different file types have (in average) different processing costs, etc. 
We could also look at the time series of a page loading, but that will
be even harder to process.


\subsection{Adapt screen size for devices}
\label{sec:adapt-screen-size}



Desktop version should use a desktop resolution, and the mobile instance
a common mobile resolution, such as 480x800.  Some pages dynamically
adapt their content to the screen size, so we don't want to mess up with
this.


\subsection{Cache: headers}
\label{sec:cache:-headers}



What do servers signal as cache durations, for different types of
resources?  This clearly impacts subsequent visits to the same page/site.


\subsection{Redirects}
\label{sec:redirects}



Many sites use redirects while fetching resources.  I believe this was
originally intended to allow restructuring of web sites without breaking
links, but from what I've seen this seems to be used as a tracking
intermediary without incurring too much data transfer (request ->
redirect -> long-time cachable resource; subsequent accesses will still
observe the redirect, but then can use their cached resources), or as a
delegation mechanism for ads.


\subsection{Which resources are shared across domains?}
\label{sec:which-resources-are}



There are several resources (such as "like" buttons, javascript, etc.)
that are shared across many sites (same URL).  What are their
properties, etc.  A possible extension could be to actually observe
content and calculate a hash to observe copies of the same file (or even
chunks...).  Is the browser allowed to cache them?


\subsection{How many resources are fetched from the same server?}
\label{sec:how-many-resources}



If many resources come from the same server (or domain?), pipelining of
transfers can improve efficiency and load time.  A possible extension
could be to have the server optionally "push" required page resources,
reducing required roundtrips.


\subsection{https run}
\label{sec:https-run}



Maybe perform a run with https instead of https.  Observe how many
domains are available via https;  how does https impact low-level
hardware metrics on the client side?


\subsection{"Unique" host, common domain}
\label{sec:unique-host-common}



There are some services that provide a custom server name, under the
umbrella of their domain name.  These are usually CDNs or CDN-like
services, such as CloudFront.  Maybe something interesting is in there? 
A bit vague what to look at.

\end{document}
